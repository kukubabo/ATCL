#######################################################################################
# EKS Dual CIDR 클러스터 구성
#   - 2020-08-20
#     . Cluster / NodeGroup 생성 yaml 의 AZ를 ap-southeast-2a / ap-southeast-2c 2개만 사용하도록 수정
#     . Nginx Ingress Controller의 Chart 버전 변경 ( 1.40.0 -> 1.41.1 ) : 1.41 버전으로 올리면서 기존 버전의 Docker 이미지 경로가 없어짐(ImgPullError발생)
#     . Nginx Ingress Controller에 Elastic IP(고정 IP)를 사용해서 적용하는 방법 추가
#######################################################################################

################################################
# [ 1. EKS 클러스터 생성 ]
#   - 작업경로 : 01.SET-EKS-ENV
#   - CIDR : 100.64.0.0/21
#     . eksctl 자동 생성시 public subnet 3개, private subnet 3개라 /24 대역이 최소 6개 필요
#     . /24 대역을 3개만 받게 된다면 subnet 먼저 생성하고 private subnet 지정해서 생성
################################################

eksctl create cluster -f 1.create-cluster.yaml


################################################
# [ 2. Dual CIDR 구성 ]
#   - 작업경로 : 01.SET-EKS-ENV
#   - CIDR : 128.0.0.0/16
#   - 순서
#     . VPC에 128.0.0.0/16 CIDR 추가
#     . 128.0.64.0/18 | 128.0.128.0/18 | 128.0.192.0/18 로 3개 subnet 생성
#     . subnet에 TAG 생성
#     . 각 AZ 별 PrivateRouteTable 에 신규 생성한 subnet 연결(associate)
#     . aws-node daemonset 에 아래 옵션 추가
#       - AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true
#       - ENI_CONFIG_LABEL_DEF=failure-domain.beta.kubernetes.io/zone
#     . 각 AZ에서 새로 생성한 Subnet을 POD 대역으로 사용하도록 ENIConfig 생성
################################################

./2.create.subcidr.sh


################################################
# [ 3. Node Group 생성 & Label 설정 ]
#   - 작업경로 : 01.SET-EKS-ENV
################################################

# create nodegroup ############ managed nodegroup 
eksctl create nodegroup -f 3.create-nodegroup-worker.managednodegroup.yaml
# create nodegroup ############ nodegroup ( node max-pods 설정 가능 )
eksctl create nodegroup -f 3.create-nodegroup-worker.nodegroup.yaml


# label node worker
kubectl get node -lrole=worker | grep -v ^NAME | awk '{print $1}' | while read name; do kubectl label node  $name node-role.kubernetes.io/worker=true; done


################################################
# [ 9. EKS 클러스터 삭제 ]
#   - 작업경로 : 01.SET-EKS-ENV
#   - Dual CIDR 수동 구성했던 자원을 지워야 클러스터를 정상 삭제할 수 있다.
################################################
# 1. nodegroup 삭제
eksctl delete nodegroup -f 3.create-nodegroup-worker.yaml --approve

# 2. 128.0.0.0/24 대역 subnet 구성 관련 자원 삭제
./4.delete.subcidr.sh

# 3. cluster 삭제
eksctl delete cluster -f 1.create-cluster.yaml


#######################################################################################
# K8S 기본 구성
#######################################################################################

################################################
# [ 1. HELM3 설치 ]
#   - 작업경로 : 02.HELM3
################################################

# 1. 설치
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh
chmod 700 get_helm.sh
./get_helm.sh

# 2. stable repo 추가
helm repo add stable https://kubernetes-charts.storage.googleapis.com/
helm search repo stable
helm repo update


################################################
# [ 2. Nginx ingress controller 설치 ]
#   - 작업경로 : 03.NGINX-INGRESS-CONTROLLER
################################################

# 1. "infra" namespace 생성(infra 관리용)
kubectl create namespace infra

# 2. deploy external service
helm install nginx-ingress-external stable/nginx-ingress -f 1.values.yaml.nginx-ingress-1.41.1.external -n infra

# 3.  deploy internal service
helm install nginx-ingress-internal stable/nginx-ingress -f 2.values.yaml.nginx-ingress-1.41.1.internal -n infra

# 4. 설치 확인 -  helm list
helm list -n infra


##### 참고.
# ingress controller 에 고정IP(Elastic IP) 를 적용하려면...
# 1) VPC의 Elastic IP(탄력적 IP) 메뉴에서 EKS에서 사용하는 AZ 개수만큼 Elastic IP를 생성하고 allocation ID(할당ID)값을 메모
# 2) 1.values.yaml.nginx-ingress-1.41.1.external-eip 파일의 263 line 에서 allocation ID를 입력 (AZ 개수만큼 , 로 구분하여 입력)
# 3) 1.values.yaml.nginx-ingress-1.41.1.external-eip 파일을 이용해서 ingress controller helm chart 배포
#    - helm install nginx-ingress-external stable/nginx-ingress -f 1.values.yaml.nginx-ingress-1.41.1.external-eip -n infra


#######################################################################################
# SAMPLE App 배포 테스트
#######################################################################################

################################################
# [ 1. apple / banana ]
#   - 작업경로 : 04.APP/01.test.apple_banana
################################################

# 배포
./01.test.apple_banana.sh

# 삭제
./02.cleanup.apple_banana.sh

################################################
# [ 2. guestbook ]
#   - 작업경로 : 04.APP/02.test.guestbook
################################################

# 배포
./01.test.guestbook.sh

# 삭제
./02.cleanup.guestbook.sh



#######################################################################################
# EKS에 GitLab + Jenkins + EFS Provisioner 구성
#######################################################################################

################################################
# [ 1. EFS 볼륨 생성 ]
################################################

1. EKS Cluster 의 VPC 를 사용하도록 생성
        => AZ 3개에 Main CIDR 대역으로 3개의 IP를 사용하게됨
        => EFS의 DNS Name 확인 : fs-39c6f358.efs.ap-southeast-2.amazonaws.com
 
2. 생성시 - Security Group 변경 ( default로 하면 EFS Provisionner에서 EFS 볼륨 사용못하니, EKS Cluster의 Security Group을 지정해야함. )
        => sg-05abc447a66a03a33 - eks-cluster-sg-skcc05599-647076920
 
################################################
# [ 2. EFS Provisioner 생성 - helm chart 배포 ]
#   - 작업경로 : 05.CICD/01.efs-provisioner-0.11.1
################################################
# 배포 대상 노드의 role=devops label 추가
kubectl label node XXXX role=devops

# helm chart 검색(search) / 다운로드(fetch)
helm search repo stable/efs-provisioner
helm fetch  stable/efs-provisioner

# helm chart 설정 파일(values.yaml.edit) 수정
# 수정사항 - values.yaml.edit 파일에서 efsFileSystemId 값을 좀 전에 생성한 EKS id 로 수정
tar -xvf efs-provisioner-0.11.1.tgz
diff values.yaml.edit efs-provisioner/values.yaml

9c9
<   deployEnv: prd
---
>   deployEnv: dev
38,40c38,40
<   efsFileSystemId: fs-39c6f358
<   awsRegion: ap-southeast-2
<   path: /efs-pv
---
>   efsFileSystemId: fs-12345678
>   awsRegion: us-east-2
>   path: /example-pv
44c44
<     isDefault: true
---
>     isDefault: false
49c49
<     reclaimPolicy: Retain
---
>     reclaimPolicy: Delete
79,80c79
< nodeSelector:
<   role: devops
---
> nodeSelector: {}

# ( infra namespace가 없을 경우 ) infra namespaces 생성
kubectl create ns infra
# efs-provisioner 설치
helm install efs-provisioner --namespace infra -f values.yaml.edit stable/efs-provisioner --version v0.11.1

....
You can provision an EFS-backed persistent volume with a persistent volume claim like below:
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-efs-vol-1
  annotations:
    volume.beta.kubernetes.io/storage-class: aws-efs
spec:
  storageClassName: aws-efs
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi
 
 
################################################
# [ 3. GITLAB 구성 ] => Helm gitlab/gitlab은 너무 무겁고, Sub-Pack 들이 많이 뜨니, Docker 버전을 Deployment로 띄우자
#   - 작업경로 : 05.CICD/02.gitlab-ce.12.10.11
################################################
kubectl apply -f 1.gitlab-configmap.yaml
kubectl apply -f 2.gitlab-pvc-svc-ingress.yaml
kubectl apply -f 3.deploy.gitlab-ce.yaml
 
 
################################################
# [ 4. Jenkins 구성 ] => helm v2.0.1
#   - 작업경로 : 05.CICD/03.jenkins
################################################

# helm chart 검색(search) / 다운로드(fetch)
helm search repo stable/jenkins --version v2.0.1
helm fetch stable/jenkins --version v2.0.1

# helm chart 설정 파일(values.yaml.edit) 수정
tar -xvf jenkins-2.0.1.tgz
diff values.yaml.edit jenkins/values.yaml

104c104
<   adminPassword: "패스워드"
---
>   # adminPassword: <defaults to random>
374c374
<     enabled: true
---
>     enabled: false
394c394
<     hostName: jenkins.ffptest.com
---
>     hostName:
422,425c422,425
<   hostAliases:
<    - ip: 172.20.112.181
<      hostnames:
<        - gitlab.ffptest.com
---
>   hostAliases: []
>   # - ip: 192.168.50.50
>   #   hostnames:
>   #     - something.local
598c598
<   storageClass: aws-efs
---
>   storageClass:
 
# jenkins 설치
helm install jenkins -n infra -f values.yaml.edit stable/jenkins --version v2.0.1
 
 
################################################
# [ 5. EKS에 sa/jenkins 에 cluster-admin 권한 부여 ]
#   - 작업경로 : 05.CICD/04.jenkins.setting
################################################
kubectl apply -f 1.ClusteRoleBinding.yaml

################################################
# [ 6. Jenkins Pipeline 구성 ]
#   - 작업경로 : 05.CICD/04.jenkins.setting , 06.APP_CICD/restapi
#   - 2.pipeline.groovy 참고해서 Jenkins Console에서 구성할 것
################################################
# 1. /etc/hosts 에 Domain 추가 ( local PC, bastion 서버 )
3.34.173.12 gitlab.ffptest.com
3.34.173.12 jenkins.ffptest.com
3.34.173.12 ffptest.com

# 2. (웹사이트) gitlab.ffptest.com 접속해서 신규 계정 생성 및 restapi project 생성

# 3. (bastion) 샘플 app 소스 경로의 파일을 gitlab에 push
cd 06.APP_CICD/restapi

git init
git remote add origin http://gitlab.ffptest.com/kukubabo/restapi.git
git add .
git commit -m "test"
git push -u origin master

# 4. Jenkis pipeline 생성 ( jenkins 계정 : admin / alskfl12~! )
# a) 'new item' 생성
#     - name 입력
#     - pileline 선택
#     - "ok" 버튼 클릭
# b) 가장 아래에 pileline 스크립트 작성
#     - 2.pipeline.groovy 파일 내용에서 주석 제외한 내용 복사해서 붙여넣기
# c) 윗부분에서 "이 빌드는 매ㅐ변수가 있습니다." 체크
#     - "매개변수 추가" 버튼 눌러서 "String Parameter" 4개 추가
#       . GIT_URL          = http://gitlab-ce.infra.svc.cluster.local/[project명]/restapi.git
#       . DOCKER_REGISTRY  = 847322629192.dkr.ecr.ap-southeast-2.amazonaws.com
#       . DOCKER_REPO      = restapi
#       . DOCKER_TAG       = 1.0
#     - "매개변수 추가" 버튼 눌러서 "Credentials Parameter" 1개 추가
#       . Name : 아무거나 입력
#       . Credential Type : Usernae with password 선택
#       . Default Value 옆에 "Add" 버튼 클릭하고 "jenkins" 선택
#       . Username / Password 에 gitlab 계정 정보 입력
#       . Dafault Value 눌러서 방금 입력한 계정 정보 선택
# d) "저장" 버튼 눌러서 pipeline 생성
# e) jenkins 화면에서 방금 생성한 pipeline 선택하고 "Build with Parameters" 선택하고 "빌드하기" 버튼 클릭
# f) 화면 새로고침해보면 왼쪽하던에 Build 번호가 확인되는데 해당 Build 번호 클릭
# g) Build 화면에서 "Console Output" 클릭하면 빌드 진행사항 확인 가능

################################################
# [ 7. Test용 RestAPI 호출 방법 ]
################################################
# 1. hosts 설정이 없을 경우 /etc/hosts 설정에 Doain 추가
3.34.173.12 gitlab.ffptest.com
3.34.173.12 jenkins.ffptest.com
3.34.173.12 ffptest.com

# 2. Rest API 호출
        # while true
        # do
        #    curl http://ffptest.com/api/get/salary/10001 | jq .
        #    sleep 1
        # done



#######################################################################################
# Prometheus / Grafana 구성
#######################################################################################

################################################
# [ 1. metric-server 설치 - 정상적으로 작동 안하는 것 같아서 확인 필요 ]
#   - 작업경로 : 07.MONITORING
#   - 참고 URL : https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/metrics-server.html
################################################

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml


################################################
# [ 2. prometheus 설치 ]
#   - 작업경로 : 07.MONITORING
#   - 참고 URL : https://www.eksworkshop.com/intermediate/240_monitoring/
################################################
# namespace 생성
kubectl create namespace prometheus

# prometheus 설치 ( w/helm )
helm install prometheus stable/prometheus \
    --namespace prometheus \
    --set alertmanager.persistentVolume.storageClass="gp2",server.persistentVolume.storageClass="gp2"

# prometheus 접속용 ingress 생성
kubectl apply -f ingress-prometheus.yaml

# ingress controller nlb 주소 확인 - IP 정보 확인하고 local PC의 hosts 파일에 prometheus 정보 추가
nslookup `kubectl -n infra get svc  nginx-ingress-external-controller -o json | jq -r '.status.loadBalancer.ingress[].hostname'` | grep ^Address | tail -1 | awk '{print $2}'

# ----- prometheus hosts set
# 1.2.3.4 prometheus.ffptest.com


################################################
# [ 3. grafana 설치 ]
#   - 작업경로 : 07.MONITORING
#   - 참고 URL : https://www.eksworkshop.com/intermediate/240_monitoring/
################################################
# namespace 생성
kubectl create namespace grafana

# Install grafana ( grafana.yaml 은 위에 설치한 prometheus data를 grafana에서 사용하기 위한 연결 설정 )
helm install grafana stable/grafana \
    --namespace grafana \
    --set persistence.storageClassName="gp2" \
    --set persistence.enabled=true \
    --set adminPassword='alskfl12~!' \
    --values grafana.yaml

# grafana 접속용 ingress 생성
kubectl apply -f ingress-grafana.yaml

# ingress controller nlb 주소 확인 - IP 정보 확인하고 local PC의 hosts 파일에 grafana 정보 추가
nslookup `kubectl -n infra get svc  nginx-ingress-external-controller -o json | jq -r '.status.loadBalancer.ingress[].hostname'` | grep ^Address | tail -1 | awk '{print $2}'

# ----- grafana hosts set
# 1.2.3.4 grafana.ffptest.com

# Grafana 대시보드 생성(샘플)
# 1. Cluster 모니터링
#    - '+' 버튼 누르고 'Import' 메뉴 클릭
#    - Grafana.com Dashboard 칸에 '3119' 입력하고 'Load' 버튼 클릭
#    - data sources 에 'Prometheus' 선택하고 'Import' 버튼 클릭

# 2. Pod 모니터링
#    - '+' 버튼 누르고 'Import' 메뉴 클릭
#    - Grafana.com Dashboard 칸에 '6417' 입력하고 'Load' 버튼 클릭
#    - dashboard 이름을 'Kubernetes Pods Monitoring'으로 수정
#    - data sources 에 'Prometheus' 선택하고 'Import' 버튼 클릭



#######################################################################################
# Logging 구성 ( AWS Elasticsearch Service 사용 )
#    - node ( fluentd ) ==> cloudwatch log ==> Elasticsearch service ==> kibana ( 로그 조회 )
#######################################################################################

################################################
# [ 1. 환경 변수 설정 ]
#   - 작업경로 : 08.LOGGING
################################################
export AWS_REGION=ap-southeast-2
export ACCOUNT_ID=`aws sts get-caller-identity | jq -r .Account`
export CLUSTER_NAME=skcc07715
export ES_DOMAIN_NAME="eks-skcc07715-logging"
export ES_VERSION="7.4"
export ES_DOMAIN_USER="admin"
export ES_DOMAIN_PASSWORD="Sktngm12#$"

################################################
# [ 2. IAM 구성 ]
#   - 작업경로 : 08.LOGGING
################################################
# Enabling IAM roles for service accounts on your cluster
eksctl utils associate-iam-oidc-provider \
    --cluster ${CLUSTER_NAME} \
    --approve

# Creating an IAM role and policy for your service account
mkdir ./logging/

cat <<EoF > ./logging/fluent-bit-policy.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "es:ESHttp*"
            ],
            "Resource": "arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}"
        }
    ]
}
EoF

cp ./logging/fluent-bit-policy.json ~/fluent-bit-policy.json
aws iam create-policy   \
  --policy-name fluent-bit-policy \
  --policy-document file://~/fluent-bit-policy.json
rm ~/fluent-bit-policy.json

# Create an IAM role
kubectl create namespace logging

eksctl create iamserviceaccount \
    --name fluent-bit \
    --namespace logging \
    --cluster ${CLUSTER_NAME} \
    --attach-policy-arn "arn:aws:iam::${ACCOUNT_ID}:policy/fluent-bit-policy" \
    --approve \
    --override-existing-serviceaccounts

# EKS 클러스터에 fluent bit 에 대한 ServiceAccount 생성 확인
kubectl -n logging describe sa fluent-bit


################################################
# [ 3. Elasticsearch Service에 신규 Domain 생성 ]
#   - 작업경로 : 08.LOGGING
################################################
# Create ES Domain
curl -sS https://www.eksworkshop.com/intermediate/230_logging/deploy.files/es_domain.json \
  | envsubst > ./logging/es_domain.json

cp ./logging/es_domain.json ~/es_domain.json
aws es create-elasticsearch-domain \
  --cli-input-json  file://~/es_domain.json
rm ~/es_domain.json

# ES 생성 확인 ( 약 12분 정도 걸림 )
while true
do
  if [ $(aws es describe-elasticsearch-domain --domain-name ${ES_DOMAIN_NAME} --query 'DomainStatus.Processing') == "false" ]
    then
      tput setaf 2; echo "[`date +%H:%M:%S`] The Elasticsearch cluster is ready"   ; tput setaf 9
	  break;
    else
      tput setaf 1; echo "[`date +%H:%M:%S`] The Elasticsearch cluster is NOT ready"; tput setaf 9
  fi
  sleep 10
done

# CONFIGURE ELASTICSEARCH ACCESS ( elasticsearch 에 접속할 수 있는 권한 부여 )
export FLUENTBIT_ROLE=$(eksctl get iamserviceaccount --cluster ${CLUSTER_NAME} --namespace logging -o json | jq '.iam.serviceAccounts[].status.roleARN' -r)
export ES_ENDPOINT=$(aws es describe-elasticsearch-domain --domain-name ${ES_DOMAIN_NAME} --output text --query "DomainStatus.Endpoint")

curl -sS -u "${ES_DOMAIN_USER}:${ES_DOMAIN_PASSWORD}" \
    -X PATCH \
    https://${ES_ENDPOINT}/_opendistro/_security/api/rolesmapping/all_access?pretty \
    -H 'Content-Type: application/json' \
    -d'
[
  {
    "op": "add", "path": "/backend_roles", "value": ["'${FLUENTBIT_ROLE}'"]
  }
]
'

################################################
# [ 4. Fluent bit 설치 ]
#   - 작업경로 : 08.LOGGING
################################################
# 설치
curl -Ss https://www.eksworkshop.com/intermediate/230_logging/deploy.files/fluentbit.yaml \
    | envsubst > ./logging/fluentbit.yaml

kubectl apply -f ./logging/fluentbit.yaml

# 확인 ( daemonset으로 각 노드에 1개씩 잘 뜨는지 확인 )
kubectl -n logging get pod -o wide


################################################
# [ 5. Kibana 에서 Index Pattern 생성 및 사용 ]
#   - 작업경로 : 08.LOGGING
################################################
# KIBANA URL 확인 후 접속(웹브라우저)
KIBANA_URL="https://$ES_ENDPOINT/_plugin/kibana"; echo $KIBANA_URL

1. 메인화면에서 "Connect to your Elasticsearch index" 클릭
2. Index pattern 에 "*fluent-bit*" 입력 후 "Next step" 클릭
3. Time Filter field name 에 "@timestamp" 선택 후 "Create Index Pattern" 클릭


################################################
# [ 9. Logging 구성 삭제 ]
#   - 작업경로 : 08.LOGGING
################################################
# 환경 변수 설정
export AWS_REGION=ap-southeast-2
export ACCOUNT_ID=`aws sts get-caller-identity | jq -r .Account`
export CLUSTER_NAME=skcc07715
export ES_DOMAIN_NAME="eks-skcc07715-logging"
export ES_VERSION="7.4"
export ES_DOMAIN_USER="admin"
export ES_DOMAIN_PASSWORD="Sktngm12#$"

# fluentbit 삭제
kubectl delete -f ./logging/fluentbit.yaml

# ES 삭제
aws es delete-elasticsearch-domain \
    --domain-name ${ES_DOMAIN_NAME}

# iam service account 삭제
eksctl delete iamserviceaccount \
    --name fluent-bit \
    --namespace logging \
    --cluster ${CLUSTER_NAME} \
    --wait

# iam policy 삭제
aws iam delete-policy   \
  --policy-arn "arn:aws:iam::${ACCOUNT_ID}:policy/fluent-bit-policy"

# logging namespace 삭제
kubectl delete namespace logging

