#######################################################################################
# EKS Dual CIDR 클러스터 구성
#######################################################################################

################################################
# [ 1. EKS 클러스터 생성 ]
#   - 작업경로 : 01.SET-EKS-ENV
#   - CIDR : 100.64.0.0/21
#     . eksctl 자동 생성시 public subnet 3개, private subnet 3개라 /24 대역이 최소 6개 필요
#     . /24 대역을 3개만 받게 된다면 subnet 먼저 생성하고 private subnet 지정해서 생성
################################################

eksctl create cluster -f 1.create-cluster.yaml


################################################
# [ 2. Dual CIDR 구성 ]
#   - 작업경로 : 01.SET-EKS-ENV
#   - CIDR : 128.0.0.0/16
#   - 순서
#     . VPC에 128.0.0.0/16 CIDR 추가
#     . 128.0.64.0/18 | 128.0.128.0/18 | 128.0.192.0/18 로 3개 subnet 생성
#     . subnet에 TAG 생성
#     . 각 AZ 별 PrivateRouteTable 에 신규 생성한 subnet 연결(associate)
#     . aws-node daemonset 에 아래 옵션 추가
#       - AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true
#       - ENI_CONFIG_LABEL_DEF=failure-domain.beta.kubernetes.io/zone
#     . 각 AZ에서 새로 생성한 Subnet을 POD 대역으로 사용하도록 ENIConfig 생성
################################################

./2.create.subcidr.sh


################################################
# [ 3. Node Group 생성 & Label 설정 ]
#   - 작업경로 : 01.SET-EKS-ENV
################################################

# create nodegroup ############ managed nodegroup 
eksctl create nodegroup -f 3.create-nodegroup-worker.managednodegroup.yaml
# create nodegroup ############ nodegroup ( node max-pods 설정 가능 )
eksctl create nodegroup -f 3.create-nodegroup-worker.nodegroup.yaml


# label node worker
kubectl get node -lrole=worker | grep -v ^NAME | awk '{print $1}' | while read name; do kubectl label node  $name node-role.kubernetes.io/worker=true; done


################################################
# [ 9. EKS 클러스터 삭제 ]
#   - 작업경로 : 01.SET-EKS-ENV
#   - Dual CIDR 수동 구성했던 자원을 지워야 클러스터를 정상 삭제할 수 있다.
################################################
# 1. nodegroup 삭제
eksctl delete nodegroup -f 3.create-nodegroup-worker.yaml --approve

# 2. 128.0.0.0/24 대역 subnet 구성 관련 자원 삭제
./4.delete.subcidr.sh

# 3. cluster 삭제
eksctl delete cluster -f 1.create-cluster.yaml


#######################################################################################
# K8S 기본 구성
#######################################################################################

################################################
# [ 1. HELM3 설치 ]
#   - 작업경로 : 02.HELM3
################################################

# 1. 설치
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh
chmod 700 get_helm.sh
./get_helm.sh

# 2. stable repo 추가
helm repo add stable https://kubernetes-charts.storage.googleapis.com/
helm search repo stable
helm repo update


################################################
# [ 2. Nginx ingress controller 설치 ]
#   - 작업경로 : 03.NGINX-INGRESS-CONTROLLER
################################################

# 1. "infra" namespace 생성(infra 관리용)
kubectl create namespace infra

# 2. deploy external service
helm install nginx-ingress-external stable/nginx-ingress -f 1.values.yaml.nginx-ingress-1.40.0.external -n infra

# 3.  deploy internal service
helm install nginx-ingress-internal stable/nginx-ingress -f 2.values.yaml.nginx-ingress-1.40.0.internal -n infra

# 4. 설치 확인 -  helm list
helm list -n infra


#######################################################################################
# SAMPLE App 배포 테스트
#######################################################################################

################################################
# [ 1. apple / banana ]
#   - 작업경로 : 04.APP/01.test.apple_banana
################################################

# 배포
./01.test.apple_banana.sh

# 삭제
./02.cleanup.apple_banana.sh

################################################
# [ 2. guestbook ]
#   - 작업경로 : 04.APP/02.test.guestbook
################################################

# 배포
./01.test.guestbook.sh

# 삭제
./02.cleanup.guestbook.sh



#######################################################################################
# EKS에 GitLab + Jenkins + EFS Provisioner 구성
#######################################################################################

################################################
# [ 1. EFS 볼륨 생성 ]
################################################

1. EKS Cluster 의 VPC 를 사용하도록 생성
        => AZ 3개에 Main CIDR 대역으로 3개의 IP를 사용하게됨
        => EFS의 DNS Name 확인 : fs-39c6f358.efs.ap-northeast-2.amazonaws.com
 
2. 생성시 - Security Group 변경 ( default로 하면 EFS Provisionner에서 EFS 볼륨 사용못하니, EKS Cluster의 Security Group을 지정해야함. )
        => sg-05abc447a66a03a33 - eks-cluster-sg-skcc05599-647076920
 
################################################
# [ 2. EFS Provisioner 생성 - helm chart 배포 ]
#   - 작업경로 : 05.CICD/01.efs-provisioner-0.11.1
################################################
# 배포 대상 노드의 role=devops label 추가
kubectl label node XXXX role=devops

# helm chart 검색(search) / 다운로드(fetch)
helm search repo stable/efs-provisioner
helm fetch  stable/efs-provisioner

# helm chart 설정 파일(values.yaml.edit) 수정
# 수정사항 - values.yaml.edit 파일에서 efsFileSystemId 값을 좀 전에 생성한 EKS id 로 수정
tar -xvf efs-provisioner-0.11.1.tgz
diff values.yaml.edit efs-provisioner/values.yaml

9c9
<   deployEnv: prd
---
>   deployEnv: dev
38,40c38,40
<   efsFileSystemId: fs-39c6f358
<   awsRegion: ap-northeast-2
<   path: /efs-pv
---
>   efsFileSystemId: fs-12345678
>   awsRegion: us-east-2
>   path: /example-pv
44c44
<     isDefault: true
---
>     isDefault: false
49c49
<     reclaimPolicy: Retain
---
>     reclaimPolicy: Delete
79,80c79
< nodeSelector:
<   role: devops
---
> nodeSelector: {}

# ( infra namespace가 없을 경우 ) infra namespaces 생성
kubectl create ns infra
# efs-provisioner 설치
helm install efs-provisioner --namespace infra -f values.yaml.edit stable/efs-provisioner --version v0.11.1

....
You can provision an EFS-backed persistent volume with a persistent volume claim like below:
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-efs-vol-1
  annotations:
    volume.beta.kubernetes.io/storage-class: aws-efs
spec:
  storageClassName: aws-efs
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi
 
 
################################################
# [ 3. GITLAB 구성 ] => Helm gitlab/gitlab은 너무 무겁고, Sub-Pack 들이 많이 뜨니, Docker 버전을 Deployment로 띄우자
#   - 작업경로 : 05.CICD/02.gitlab-ce.12.10.11
################################################
kubectl apply -f 1.gitlab-configmap.yaml
kubectl apply -f 2.gitlab-pvc-svc-ingress.yaml
kubectl apply -f 3.deploy.gitlab-ce.yaml
 
 
################################################
# [ 4. Jenkins 구성 ] => helm v2.0.1
#   - 작업경로 : 05.CICD/03.jenkins
################################################

# helm chart 검색(search) / 다운로드(fetch)
helm search repo stable/jenkins --version v2.0.1
helm fetch stable/jenkins --version v2.0.1

# helm chart 설정 파일(values.yaml.edit) 수정
tar -xvf jenkins-2.0.1.tgz
diff values.yaml.edit jenkins/values.yaml

104c104
<   adminPassword: "패스워드"
---
>   # adminPassword: <defaults to random>
374c374
<     enabled: true
---
>     enabled: false
394c394
<     hostName: jenkins.ffptest.com
---
>     hostName:
422,425c422,425
<   hostAliases:
<    - ip: 172.20.112.181
<      hostnames:
<        - gitlab.ffptest.com
---
>   hostAliases: []
>   # - ip: 192.168.50.50
>   #   hostnames:
>   #     - something.local
598c598
<   storageClass: aws-efs
---
>   storageClass:
 
# jenkins 설치
helm install jenkins -n infra -f values.yaml.edit stable/jenkins --version v2.0.1
 
 
################################################
# [ 5. EKS에 sa/jenkins 에 cluster-admin 권한 부여 ]
#   - 작업경로 : 05.CICD/04.jenkins.setting
################################################
kubectl apply -f 1.ClusteRoleBinding.yaml

################################################
# [ 6. Jenkins Pipeline 구성 ]
#   - 작업경로 : 05.CICD/04.jenkins.setting , 06.APP_CICD/restapi
#   - 2.pipeline.groovy 참고해서 Jenkins Console에서 구성할 것
################################################
# 1. /etc/hosts 에 Domain 추가 ( local PC, bastion 서버 )
3.34.173.12 gitlab.ffptest.com
3.34.173.12 jenkins.ffptest.com
3.34.173.12 ffptest.com

# 2. (웹사이트) gitlab.ffptest.com 접속해서 신규 계정 생성 및 restapi project 생성

# 3. (bastion) 샘플 app 소스 경로의 파일을 gitlab에 push
cd 06.APP_CICD/restapi

git init
git remote add origin http://gitlab.ffptest.com/kukubabo/restapi.git
git add .
git commit -m "test"
git push -u origin master

# 4. Jenkis pipeline 생성 ( jenkins 계정 : admin / alskfl12~! )
# a) 'new item' 생성
#     - name 입력
#     - pileline 선택
#     - "ok" 버튼 클릭
# b) 가장 아래에 pileline 스크립트 작성
#     - 2.pipeline.groovy 파일 내용에서 주석 제외한 내용 복사해서 붙여넣기
# c) 윗부분에서 "이 빌드는 매ㅐ변수가 있습니다." 체크
#     - "매개변수 추가" 버튼 눌러서 "String Parameter" 4개 추가
#       . GIT_URL          = http://gitlab-ce.infra.svc.cluster.local/[project명]/restapi.git
#       . DOCKER_REGISTRY  = 847322629192.dkr.ecr.ap-northeast-2.amazonaws.com
#       . DOCKER_REPO      = restapi
#       . DOCKER_TAG       = 1.0
#     - "매개변수 추가" 버튼 눌러서 "Credentials Parameter" 1개 추가
#       . Name : 아무거나 입력
#       . Credential Type : Usernae with password 선택
#       . Default Value 옆에 "Add" 버튼 클릭하고 "jenkins" 선택
#       . Username / Password 에 gitlab 계정 정보 입력
#       . Dafault Value 눌러서 방금 입력한 계정 정보 선택
# d) "저장" 버튼 눌러서 pipeline 생성
# e) jenkins 화면에서 방금 생성한 pipeline 선택하고 "Build with Parameters" 선택하고 "빌드하기" 버튼 클릭
# f) 화면 새로고침해보면 왼쪽하던에 Build 번호가 확인되는데 해당 Build 번호 클릭
# g) Build 화면에서 "Console Output" 클릭하면 빌드 진행사항 확인 가능

################################################
# [ 7. Test용 RestAPI 호출 방법 ]
################################################
# 1. hosts 설정이 없을 경우 /etc/hosts 설정에 Doain 추가
3.34.173.12 gitlab.ffptest.com
3.34.173.12 jenkins.ffptest.com
3.34.173.12 ffptest.com

# 2. Rest API 호출
        # while true
        # do
        #    curl http://ffptest.com/api/get/salary/10001 | jq .
        #    sleep 1
        # done



#######################################################################################
# Prometheus / Grafana 구성
#######################################################################################

################################################
# [ 1. metric-server 설치 - 정상적으로 작동 안하는 것 같아서 확인 필요 ]
#   - 작업경로 : 07.MONITORING
#   - 참고 URL : https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/metrics-server.html
################################################

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml


################################################
# [ 2. prometheus 설치 ]
#   - 작업경로 : 07.MONITORING
#   - 참고 URL : https://www.eksworkshop.com/intermediate/240_monitoring/
################################################
# namespace 생성
kubectl create namespace prometheus

# prometheus 설치 ( w/helm )
helm install prometheus stable/prometheus \
    --namespace prometheus \
    --set alertmanager.persistentVolume.storageClass="gp2",server.persistentVolume.storageClass="gp2"

# prometheus 접속용 ingress 생성
kubectl apply -f ingress-prometheus.yaml

# ingress controller nlb 주소 확인 - IP 정보 확인하고 local PC의 hosts 파일에 prometheus 정보 추가
nslookup `kubectl -n infra get svc  nginx-ingress-external-controller -o json | jq -r '.status.loadBalancer.ingress[].hostname'` | grep ^Address | tail -1 | awk '{print $2}'

# ----- prometheus hosts set
# 1.2.3.4 prometheus.ffptest.com


################################################
# [ 3. grafana 설치 ]
#   - 작업경로 : 07.MONITORING
#   - 참고 URL : https://www.eksworkshop.com/intermediate/240_monitoring/
################################################
# namespace 생성
kubectl create namespace grafana

# Install grafana ( grafana.yaml 은 위에 설치한 prometheus data를 grafana에서 사용하기 위한 연결 설정 )
kubectl create namespace grafana
helm install grafana stable/grafana \
    --namespace grafana \
    --set persistence.storageClassName="gp2" \
    --set persistence.enabled=true \
    --set adminPassword='alskfl12~!' \
    --values grafana.yaml

# grafana 접속용 ingress 생성
kubectl apply -f ingress-grafana.yaml

# ingress controller nlb 주소 확인 - IP 정보 확인하고 local PC의 hosts 파일에 grafana 정보 추가
nslookup `kubectl -n infra get svc  nginx-ingress-external-controller -o json | jq -r '.status.loadBalancer.ingress[].hostname'` | grep ^Address | tail -1 | awk '{print $2}'

# ----- grafana hosts set
# 1.2.3.4 grafana.ffptest.com



#######################################################################################
# Logging 구성 ( AWS Elasticsearch Service 사용 )
#    - node ( fluentd ) ==> cloudwatch log ==> Elasticsearch service ==> kibana ( 로그 조회 )
#######################################################################################

################################################
# [ 1. IAM ROLE 설정 ]
#   - 작업경로 : 08.LOGGING
################################################

# NODE GROUP 에서 사용하는 IAM ROLE 정보 확인
CLUSTER_NAME=skcc07715
STACK_NAME=$(eksctl get nodegroup --cluster $CLUSTER_NAME -o json | jq -r '.[].StackName')
ROLE_NAME=$(aws cloudformation describe-stack-resources --stack-name $STACK_NAME | jq -r '.StackResources[] | select(.ResourceType=="AWS::IAM::Role") | .PhysicalResourceId')
echo "export ROLE_NAME=${ROLE_NAME}" | tee -a ~/.bash_profile

# ROLE POLICY에 정책 추가
cp iam_policy/k8s-logs-policy.json  ~/
aws iam put-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker --policy-document file://~/k8s-logs-policy.json
rm ~/k8s-logs-policy.json

# ROLE POLICY 확인
aws iam get-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker


################################################
# [ 2. AWS Elasticsearch Service 도메인 생성 ]
#   - 작업경로 : 08.LOGGING
#                - 도메인명 : kubernetes-logs
#                - 버전     : 7.4 ( 2020/07/22 기준 최신버전 )
#                - 스펙     : m5.large 인스턴스 2개로 Elasticsearch 구성(100Gb)
################################################

# Elasticsearch Service 생성(약 10분 소요)
aws es create-elasticsearch-domain \
  --domain-name kubernetes-logs \
  --elasticsearch-version 7.4 \
  --elasticsearch-cluster-config \
  InstanceType=m5.large.elasticsearch,InstanceCount=2 \
  --ebs-options EBSEnabled=true,VolumeType=standard,VolumeSize=100 \
  --access-policies '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"AWS":["*"]},"Action":["es:*"],"Resource":"*"}]}'

# 생성 확인 ( 결과 값이 false 가 나와야 정상 )
aws es describe-elasticsearch-domain --domain-name kubernetes-logs --query 'DomainStatus.Processing'


################################################
# [ 3. Fluentd 설치 ( 각 노드에서 로그 데이터를 CloudWatch Log 로 쏴주는 역할 ) ]
#   - 작업경로 : 08.LOGGING
################################################

# 리전 / EKS 클러스터명 수정
vi fluentd/fluentd.yml 하여 199~202 line 내용 수정
198:    env:
199:      - name: REGION
200:        value: us-west-2
201:      - name: CLUSTER_NAME
202:        value: eksworkshop-eksctl

kubectl apply -f fluentd/fluentd.yml


################################################
# [ 4. CloudWatch 설정 - ES 로 보내는 설정인듯?? ]
#   - 작업경로 : 08.LOGGING
################################################

# IAM 권한 부여???
cp iam_policy/lambda.json  ~/
aws iam create-role --role-name lambda_basic_execution --assume-role-policy-document file://~/lambda.json
aws iam attach-role-policy --role-name lambda_basic_execution --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
rm ~/lambda.json

# CloudWatch Log 웹콘솔에서
1. 로그 그룹에서 /eks/클러스터명/containers 체크하고 "작업(Actions)"을 눌러 "Amazon Elasticsearch Service 측 스트림(Stream to Amazon ElasticSearch Service)" 선택
2. 다음 화면에서 ElasticSearch Cluster명으로 "kubernetes-logs" 선택 / IAM role "lambda_basic_excution" 선택
3. Log Format 을 "일반적 로그 형식(Common Log Format)" 으로 선택  =====> 로그 포멧이 좀 맞지 않아서 Customize 필요해 보임
4. Review 후 Start Streaming

# Kibana 설정
1. Elasticsearch Service 웹콘솔에서 kubernetes-logs 정보 확인하면 Kibana 접속링크가 있음 ( 링크로 이동 )
2. 왼쪽 아래에 "Management" 메뉴 이동 후 "Index Patterns" - "Create Index pattern" 클릭
3. Index Pattern 에 "cwl-*" 입력 후 다음
4. Time Fileter field name 에서 "@timestamp" 선택 후 "Create Index Pattern" 으로 생성
5. 왼쪽 상단에 "Discover" 메뉴로 가면 로그 조회 가능








################################################
# [ 5. 제거하기 ]
#   - 작업경로 : 08.LOGGING
################################################
kubectl delete -f ./fluentd/fluentd.yml
aws es delete-elasticsearch-domain --domain-name kubernetes-logs
aws logs delete-log-group --log-group-name /eks/skcc07715/containers

역할 - NodeInstanceRole 에서 인라인 정책에 "Logs-Policy-For-Worker" 제거
역할 - lambda_basic_execution 삭제

